# -*- coding: utf-8 -*-
"""ImageClassificationViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ryTq44l4q7_96dUnXsBMZrYZYDf8-A5
"""

import torch
import matplotlib.pyplot as plt
import torchvision

from torch import nn
from torchvision import transforms

device = 'mps'
train_dir = '/Users/hiteshgupta/Documents/ML-CV/Vision Transformer/dataset/train'
val_dir = '/Users/hiteshgupta/Documents/ML-CV/Vision Transformer/dataset/val'

"""# Create Datasets and DataLoaders"""

import os
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

def create_dataloaders(train_dir: str, val_dir: str, transform: transforms.Compose, batch_size: int, num_workers: int):
    # Use image folder to create dataset(s)
    train_data = datasets.ImageFolder(train_dir, transform = transform)
    val_data = datasets.ImageFolder(val_dir, transform = transform)

    # Get Class Names
    class_names = train_data.classes

    # Turn images into data loaders
    train_dataloader = DataLoader(
        train_data,
        batch_size = batch_size,
        shuffle = True,
        num_workers = num_workers,
        pin_memory = True,
    )
    val_dataloader = DataLoader(
        val_data,
        batch_size = batch_size,
        shuffle = True,
        num_workers = num_workers,
        pin_memory = True,
    )

    return train_dataloader, val_dataloader, class_names

# Create image size
IMG_SIZE = 224

# Create transform pipeline manually
manual_transforms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
])
print(f'Manully Created transforms: {manual_transforms}')

BATCH_SIZE = 32

# Create data loaders
train_dataloader, val_dataloader, class_names = create_dataloaders(
    train_dir,
    val_dir,
    manual_transforms,
    BATCH_SIZE,
    num_workers = 8
)

print(f'Class Names: {class_names}')
print(f'Length of Train Dataloader: {len(train_dataloader)}')
print(f'Length of Val Dataloader: {len(val_dataloader)}')

# Let's visualize a image in order to know if data is loaded properly or not

# # Get a batch of images
# image_batch, label_batch = next(iter(train_dataloader))

# # Get a single image from the batch
# image, label = image_batch[0], label_batch[0]

# # View the batch shapes
# print(image.shape, label)

# # Plot image with matplotlib
# plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]
# plt.title(class_names[label])
# plt.axis(False);

class PatchEmbedding(nn.Module):
    """Turn a 2D input image into a 1D sequence learnable embedding vectors.

    Args:
        in_channels (int): Number of input channels
        patch_size (int): The size of the square patch to be created
        emb_size (int): The embedding size of each patch
    """
    # Initialize the class
    def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768): # 16*16*3 = 768
        super().__init__()

        # Break image into patches
        self.patcher = nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=patch_size, stride=patch_size, padding=0)

        # Create a layer to flatten the image into a sequence
        self.flatten = nn.Flatten(start_dim=2, end_dim=3)

    # Forward pass
    def forward(self, x):
        # Create assetion to check the input image size
        image_res = x.shape[-1]
        assert image_res % patch_size == 0, f"Image resolution {image_res} must be divisible by {patch_size}"

        # perform the forward pass
        x_patched = self.patcher(x)
        x_flattened = self.flatten(x_patched)

        return x_flattened.permute(0, 2, 1)

def set_seeds(seed : int = 42):
    torch.manual_seed(seed)
    torch.mps.manual_seed(0)

set_seeds()
patch_size = 16
# Create an instance of the PatchEmbedding class
# patchify = PatchEmbedding(in_channels=3, patch_size=patch_size, emb_size=768)

# print(f"Imput Image Shape : {image.unsqueeze(0).shape}")
# patched_embedded_image = patchify(image.unsqueeze(0))
# print(f"Output Shape: {patched_embedded_image.shape}")

# print(f'Image Tensor Shape : {image.shape}')
# height, width = image.shape[1], image.shape[2]

# #Get image tensor and add batch dimensions
# x = image.unsqueeze(0)
# print(f"Input image with batch dimension shape:{x.shape}")

# # Pass image through patch embedding layer
# patch_embedding = patchify(x)
# print(f"Patching embedding shape : {patch_embedding.shape}")

# # Create class token embedding
# batch_size = patch_embedding.shape[0]
# embedding_dimension = patch_embedding.shape[-1]
# class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), requires_grad=True)
# print(f"Class token embedding shape : {class_token.shape}")

# # Prepend class token embedding to patch embedding
# patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim =1)
# print(f"Patch embedding with class token shape : {patch_embedding_class_token.shape}")

# # Create position embedding
# number_of_patches = int((height*width)/patch_size**2)
# position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension), requires_grad=True)

# # Add position embedding to patch embedding with class token
# patch_and_position_embedding = patch_embedding_class_token + position_embedding
# print(f"Patch and embedding shape : {patch_and_position_embedding.shape}")

# print(patch_embedding_class_token)

"""# Creating Vision Transformer

## MultiHead self Attention
"""

class MultiheadSelfAttention(nn.Module):
    def __init__(self, embedding_dim:int = 768, num_heads:int = 12, attn_dropout:float = 0):
        super().__init__()

        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=attn_dropout, batch_first=True)

    def forward(self, x):
        x = self.layer_norm(x)
        attn_output, _ = self.multihead_attn(query =x, key = x, value = x, need_weights = False)
        return attn_output

"""# MLP Block"""

class MLPBlock(nn.Module):
    def __init__(self, embedding_dim:int = 768, mlp_size: int=3072, dropout : float = 0.1):
        super().__init__()

        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)

        self.mlp = nn.Sequential(
            nn.Linear(in_features=embedding_dim,
                      out_features=mlp_size),
            nn.GELU(),
            nn.Dropout(p = dropout),
            nn.Linear(in_features = mlp_size, out_features = embedding_dim),
            nn.Dropout(p = dropout)
        )

    def forward(self, x):
        x = self.layer_norm(x)
        x = self.mlp(x)
        return x

"""## Creating a Transformer encoder by combining our custom made layers"""

class TransformerEncoderBlock(nn.Module):
    def __init__(self, embedding_dim: int = 768, num_heads: int= 12, mlp_size: int = 3072, mlp_dropout: float = 0.1, attn_dropout: float = 0):
        super().__init__()
        # Create MSA Block
        self.msa_block = MultiheadSelfAttention(embedding_dim = embedding_dim, num_heads=num_heads, attn_dropout = attn_dropout)

        # Create MLP block
        self.mlp_block = MLPBlock(embedding_dim = embedding_dim, mlp_size = mlp_size, dropout=mlp_dropout)

    def forward(self, x):
        x = self.msa_block(x) + x
        x = self.mlp_block(x) + x
        return x

transformer_encoder_block = TransformerEncoderBlock()

from torchinfo import summary

summary(model = transformer_encoder_block, col_name = ["input_size", "output_size", "num_params", "trainable"],
        col_width = 20, row_settings = ["var_names"])

"""# Lets Build the VISION TRANSFORMER"""

class ViT(nn.Module):
    def __init__(self, img_size: int = 224,
                 in_channels: int = 3,
                 patch_size: int = 16,
                 num_transformers_layers: int = 12,
                 embedding_dim: int = 768,
                 mlp_size: int = 3072,
                 num_heads: int = 12,
                 attn_dropout: float = 0,
                 mlp_dropout : float = 0.1,
                 embedding_dropout: float = 0.1,
                 num_classes: int = 1000):
        super().__init__()

        assert img_size % patch_size == 0, f"Image size must be divisible by patch size, image size:{img_size}, patch size = {patch_size}"

        self.num_patches = (img_size * img_size) // (patch_size **2)

        self.class_embedding = nn.Parameter(data = torch.randn(1,1,embedding_dim), requires_grad=True)

        self.position_embedding = nn.Parameter(data = torch.randn(1, self.num_patches + 1, embedding_dim), requires_grad = True)

        self.embedding_dropout = nn.Dropout(p = embedding_dropout)

        self.patch_embedding = PatchEmbedding(in_channels = in_channels,patch_size=patch_size, emb_size=embedding_dim)

        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim = embedding_dim,
                                                                           num_heads=num_heads,
                                                                           mlp_size=mlp_size,
                                                                           mlp_dropout=mlp_dropout) for _ in range(num_transformers_layers)])

        self.classifier = nn.Sequential(
            nn.LayerNorm(normalized_shape=embedding_dim),
            nn.Linear(in_features=embedding_dim, out_features=num_classes)
        )

    def forward(self, x):
        batch_size = x.shape[0]

        class_token = self.class_embedding.expand(batch_size, -1, -1)

        x = self.patch_embedding(x)

        x = torch.cat((class_token, x), dim = 1)

        x = self.position_embedding + x

        x = self.embedding_dropout(x)

        x = self.transformer_encoder(x)

        x = self.classifier(x[:,0])

        return x


# Create a function to calculate the accuracy of the model
def calculate_accuracy(y_true, y_pred):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)

    correct_pred = (y_pred_tags == y_true).float()
    acc = correct_pred.sum() / len(correct_pred)
    return acc

# Create a function to train the model
def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, device):
    # Initialize a list to store the loss values
    train_loss = []
    val_loss = []

    # Initialize a list to store the accuracy values
    train_acc = []
    val_acc = []

    # Loop through the epochs
    for epoch in range(num_epochs):
        # Set the model to train mode
        model.train()

        # Initialize the running loss and accuracy values
        running_train_loss = 0.0
        running_train_acc = 0.0

        # Loop through the training data
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Get the predictions
            outputs = model(images)

            # Calculate the loss
            loss = criterion(outputs, labels)

            # Backpropagate
            loss.backward()

            # Update the weights
            optimizer.step()

            # Add the loss to the running loss
            running_train_loss += loss.item()

            # Calculate the accuracy
            acc = calculate_accuracy(labels, outputs)

            # Add the accuracy to the running accuracy
            running_train_acc += acc

        # Calculate the average loss and accuracy
        avg_train_loss = running_train_loss / len(train_loader)
        avg_train_acc = running_train_acc / len(train_loader)

        # Append the loss and accuracy to the respective lists
        train_loss.append(avg_train_loss)
        train_acc.append(avg_train_acc)

        # Set the model to evaluation mode
        model.eval()

        # Initialize the running loss and accuracy values
        running_val_loss = 0.0
        running_val_acc = 0.0

        # Loop through the validation data
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)

            # Get the predictions
            outputs = model(images)

            # Calculate the loss
            loss = criterion(outputs, labels)

            # Add the loss to the running loss
            running_val_loss += loss.item()

            # Calculate the accuracy
            acc = calculate_accuracy(labels, outputs)

            # Add the accuracy to the running accuracy
            running_val_acc += acc

        # Calculate the average loss and accuracy
        avg_val_loss = running_val_loss / len(val_loader)
        avg_val_acc = running_val_acc
    
        # Append the loss and accuracy to the respective lists
        val_loss.append(avg_val_loss)
        val_acc.append(avg_val_acc)
        
        # Print the epoch, training loss, validation loss, training accuracy, and validation accuracy
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Train Acc: {avg_train_acc:.4f}, Val Acc: {avg_val_acc:.4f}")
        
    return train_loss, val_loss, train_acc, val_acc

# Train the model
# Set seed for reproducibility
set_seeds()

# Create an instance of the ViT model

vit_model = ViT()

# Move the model to the GPU
vit_model.to(device)

# Define the criterion
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = torch.optim.Adam(vit_model.parameters(), lr=0.0001)

# Define the number of epochs
num_epochs = 10

# Train the model
if __name__ == '__main__':
    train_loss, val_loss, train_acc, val_acc = train_model(vit_model, criterion, optimizer, train_dataloader, val_dataloader, num_epochs, device)
    plt.figure(figsize=(10, 5))
    plt.plot(train_loss, label="Train Loss", color="blue")
    plt.plot(val_loss, label="Val Loss", color="red")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Validation Loss")
    plt.legend()
    plt.show()

    # Plot the training and validation accuracy
    plt.figure(figsize=(10, 5))
    plt.plot(train_acc, label="Train Acc", color="blue")
    plt.plot(val_acc, label="Val Acc", color="red")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Training and Validation Accuracy")
    plt.legend()
    plt.show()
    
    # Save the model
    torch.save(vit_model.state_dict(), "vit_model.pth")