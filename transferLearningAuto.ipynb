{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9585b0e-e065-4e95-b1e1-7a5d27aa5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383e1f44-ab1a-4629-85d0-c3e1ae1e619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "train_dir = '/Users/hiteshgupta/Documents/ML-CV/Vision Transformer/ImageForgery/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26048350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, \n",
    "                               kernel_size=3, \n",
    "                               stride=2, \n",
    "                               padding=1, \n",
    "                               output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8f4077-943d-4d67-b91c-8a4d1bafa501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (pretrained_vit): VisionTransformer(\n",
       "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (encoder): Encoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): Sequential(\n",
       "        (encoder_layer_0): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_1): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_2): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_3): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_4): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_5): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_6): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_7): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_8): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_9): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_10): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (encoder_layer_11): EncoderBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (self_attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (heads): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (autoencoder): Autoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU()\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a model with autoencoder and pretrained Vision Transform\n",
    "model = Autoencoder()\n",
    "\n",
    "\n",
    "# Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, pretrained_vit_weights):\n",
    "        super(ViT, self).__init__()\n",
    "        self.pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "        self.autoencoder = Autoencoder().to(device)\n",
    "        for parameter in self.pretrained_vit.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        \n",
    "        self.pretrained_vit.heads = nn.Linear(in_features=768, out_features=2).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.autoencoder(x)\n",
    "        x = self.pretrained_vit(x)\n",
    "        return x\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "# pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# # 3. Freeze the base parameters\n",
    "# for parameter in pretrained_vit.parameters():\n",
    "#     parameter.requires_grad = False\n",
    "\n",
    "pretrained_vit = ViT(pretrained_vit_weights)\n",
    "    \n",
    "# 4. Change the classifier head \n",
    "class_names =  ['forged', 'real']\n",
    "\n",
    "# set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "pretrained_vit # uncomment for model output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce7bc4b8-437a-4018-9338-ee2a96ea4a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "ViT (ViT)                                                         [32, 3, 224, 224]    [32, 2]              1,538                Partial\n",
       "├─Autoencoder (autoencoder)                                       [32, 3, 224, 224]    [32, 3, 224, 224]    --                   True\n",
       "│    └─Sequential (encoder)                                       [32, 3, 224, 224]    [32, 8, 56, 56]      --                   True\n",
       "│    │    └─Conv2d (0)                                            [32, 3, 224, 224]    [32, 16, 224, 224]   448                  True\n",
       "│    │    └─ReLU (1)                                              [32, 16, 224, 224]   [32, 16, 224, 224]   --                   --\n",
       "│    │    └─MaxPool2d (2)                                         [32, 16, 224, 224]   [32, 16, 112, 112]   --                   --\n",
       "│    │    └─Conv2d (3)                                            [32, 16, 112, 112]   [32, 8, 112, 112]    1,160                True\n",
       "│    │    └─ReLU (4)                                              [32, 8, 112, 112]    [32, 8, 112, 112]    --                   --\n",
       "│    │    └─MaxPool2d (5)                                         [32, 8, 112, 112]    [32, 8, 56, 56]      --                   --\n",
       "│    └─Sequential (decoder)                                       [32, 8, 56, 56]      [32, 3, 224, 224]    --                   True\n",
       "│    │    └─ConvTranspose2d (0)                                   [32, 8, 56, 56]      [32, 16, 112, 112]   1,168                True\n",
       "│    │    └─ReLU (1)                                              [32, 16, 112, 112]   [32, 16, 112, 112]   --                   --\n",
       "│    │    └─ConvTranspose2d (2)                                   [32, 16, 112, 112]   [32, 3, 224, 224]    435                  True\n",
       "│    │    └─Sigmoid (3)                                           [32, 3, 224, 224]    [32, 3, 224, 224]    --                   --\n",
       "├─VisionTransformer (pretrained_vit)                              [32, 3, 224, 224]    [32, 2]              768                  Partial\n",
       "│    └─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
       "│    └─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
       "│    │    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    │    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       (85,054,464)         False\n",
       "│    │    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
       "│    └─Linear (heads)                                             [32, 768]            [32, 2]              1,538                True\n",
       "=================================================================================================================================================\n",
       "Total params: 85,804,943\n",
       "Trainable params: 6,287\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.GIGABYTES): 7.87\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3651.86\n",
       "Params size (MB): 229.21\n",
       "Estimated Total Size (MB): 3900.34\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit, \n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68db1528-1ae4-438a-bf43-a1851fc3c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/Users/hiteshgupta/Documents/ML-CV/Vision Transformer/ImageForgery/train'\n",
    "val_dir = '/Users/hiteshgupta/Documents/ML-CV/Vision Transformer/ImageForgery/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50995e6-0d46-46dc-a048-590baf2442e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0018802-4475-4d75-b174-9f713521bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(val_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6166eb0d-ac6d-4df6-b27e-0acbd9d149d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=val_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3172c2c2-aba8-4947-9012-b406570d04cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf1237fea3e47378a3dc9fdc0344bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6945 | train_acc: 0.5388 | test_loss: 0.6990 | test_acc: 0.5120\n",
      "Epoch: 2 | train_loss: 0.6986 | train_acc: 0.5302 | test_loss: 0.7277 | test_acc: 0.4522\n",
      "Epoch: 3 | train_loss: 0.6791 | train_acc: 0.5739 | test_loss: 0.6875 | test_acc: 0.5538\n",
      "Epoch: 4 | train_loss: 0.6703 | train_acc: 0.5861 | test_loss: 0.7149 | test_acc: 0.5323\n",
      "Epoch: 5 | train_loss: 0.6671 | train_acc: 0.6000 | test_loss: 0.6025 | test_acc: 0.6767\n",
      "Epoch: 6 | train_loss: 0.6560 | train_acc: 0.6097 | test_loss: 0.6647 | test_acc: 0.6080\n",
      "Epoch: 7 | train_loss: 0.6543 | train_acc: 0.6164 | test_loss: 0.7227 | test_acc: 0.5248\n",
      "Epoch: 8 | train_loss: 0.6468 | train_acc: 0.6195 | test_loss: 0.6737 | test_acc: 0.6218\n",
      "Epoch: 9 | train_loss: 0.6513 | train_acc: 0.6225 | test_loss: 0.6052 | test_acc: 0.6794\n",
      "Epoch: 10 | train_loss: 0.6453 | train_acc: 0.6288 | test_loss: 0.6545 | test_acc: 0.5939\n",
      "Epoch: 11 | train_loss: 0.6404 | train_acc: 0.6319 | test_loss: 0.6251 | test_acc: 0.6555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x168c90a40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 947, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hiteshgupta/anaconda3/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "# set_seeds()\n",
    "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=15,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006827f4-f72f-4e50-9970-ac1d86b50145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(pretrained_vit_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a2207-d1f1-4c72-8e19-42aa9be3bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to make predictions on images and plot them \n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = \"Testpics/test3.jpg\"\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289733e3-1f6f-4bb8-bbbe-1cbb168b0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from going_modular.going_modular.utils import save_model\n",
    "save_model(model=pretrained_vit,\n",
    "           target_dir='models',\n",
    "           model_name='ImageForgeryViTEncoderPre_epochs15.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f017fd-753b-4546-bc5e-ebab6b9f36bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
